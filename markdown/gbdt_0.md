# GBDT算法详解（一）：决策树与Boosting算法

系列文章分为两部分：

1. GBDT算法详解（一）：决策树与Boosting算法
2. GBDT算法详解（二）：GBDT与Xgboost

---

[TOC]

GBDT（Gradient Boosting Descision Tree），是各类机器学习算法比赛的赢家们都非常青睐的算法，也是效果最好的统计学习方法之一。

本文及后文将从决策树和Boosting算法的基础知识开始，详细的推导GBDT的原理。如果不足，还望互相讨论。

## 1. 机器学习基础
依照《统计学习方法》一书所说，统计学习的三要素是：**模型+策略+算法**。

针对监督学习而言，

- 模型：所要学习的条件概率$P(Y|X)$或是决策函数$f(x)$。
- 策略：损失函数/目标函数
- 算法：学习模型的具体计算方法，如梯度下降


## 2. 决策树

决策树是一种基本的分类和回归方法，它的模型呈树形结构。
对于决策树，我们可以认为它是**定义在特征空间和类空间上的条件概率分布** 

以分类决策树为例，决策树的每条路径对应划分中的一个单元(cell)，在每个单元上定义一个类的概率分布就构成了一个条件概率分布。

设$X$表示特征的随机变量，$Y$表示类的随机变量，则这个条件概率分布就是$P(Y|X)$。$X$取值自单元的集合，$Y$取值自类的集合。通常各叶节点（单元）上的条件概率分布往往偏向于某个类，决策树分类时会将该节点的实例强行分到条件概率大的那一类。

### 2.1 决策树学习

决策树学习的本质是从训练数据集中归纳出一组分类规则。能正确分类训练数据集的决策树可能有多个，也可能一个没有。我们需要一个分类错误少，且具有良好泛化能力的决策树。

由于从所有可能决策树中选择最优决策树是NP完全问题，所以现实生活中决策树学习算法通常采用**启发式方法**，近似求解最优化问题。这样得到的决策树是**次最优sub-optimal**。



决策树的学习包括三个过程：

1. 特征选择
2. 决策树的生成（考虑局部最优）
3. 决策树的剪枝（考虑全局最优）
    1. 预剪枝（Pre-Pruning）：在决策树生成同时进行剪枝
    2. 后剪枝（Post-Pruning）：在决策树构造完成后进行剪枝（通常后剪枝效果好于预剪枝，因此后面该节讨论的都是后剪枝）



决策树生成的具体步骤如下：

1. 选择一个最优特征（使得各个子集有一个在当前条件下最好的分类），然后按照该特征划分数据集。
2. 如果这些子集能够基本正确分类，则构建叶节点
3. 否则，对这些子集继续选择新的最优特征，然后继续分割
4. 循环往复，直到所有训练数据子集已经被基本正确分类，或没有合适的特征为止

可知，在生成决策树时，我们采取的是【贪心】的方法。



对于上述生成步骤生成的决策树，对于训练数据拟合会很好，但是泛化能力却很差，有过拟合的问题，因此需要剪枝步骤来防止过拟合。

而决策树的剪枝步骤，则是通过极小化决策树的整体损失函数（loss function）来实现的。常见损失函数的形式为
$$
C_{\alpha}=C(T)+\alpha|T| \\
C(T)是模型对训练数据的误差，|T|是模型复杂度
$$
而剪枝，就是在$\alpha$确定时，选择令损失函数最小的模型。具体来说，就是通过递归的从树的叶子结点往上缩，判断剪枝后的树是否比未剪枝的树的$C_{\alpha}$值小，小则剪枝成功。

### 2.2 CART: Classification and Regression Tree

CART树假定决策树是**二叉树**，内部节点特征取值是“是”或“否”。这就等价于递归的二分每个特征，将输入空间即特征空间划分为有限个单元，在这些单元上预测概率分布。

CART的算法步骤也分为两步，分别是决策树生成和决策树剪枝。

在决策树生成时，CART算法要求生成尽可能大的决策树。对回归树来说，通过最小化某个$loss$函数来选择最优切分变量和最优切分值；分类树则以基尼指数来选择最有特征和最优二值切分点。

决策树剪枝时，其思想和普通决策树类似，区别选择额外的交叉验证集来选择最终的最优决策树[^1]。

#### 2.2.1 最小二乘回归树的生成

以最小二乘回归树（least squares regression tree）为例，我们来了解下决策树的生成步骤。

设数据集为$D=\{(x_1,y_1),(x_2,y_2)\dots(x_n,y_n)\}$，设树已有$M$个叶子结点$R_1,R_2,\dots,R_M$，每个叶子结点有个固定的输出值$c_m$，那么决策树模型就可表示为
$$
f(x)=\sum^M_{m=1}c_mI(x\in R_m)
$$
最小二乘回归树使用平方误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$来表示预测误差，我们需要求解出每个叶子节点上的最优输出值$\hat{c}_m$。对于平方误差求解最小值，易得$\hat{c}_m=ave(y_i|x_i\in R_m)$.

关键在于如何对输入空间进行划分？我们需要寻找最优切分变量$j$和最优切分点$s$。即求解
$$
min_{j,s}[min_{c1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
我们需要遍历所有变量$j$，然后固定$j$遍历切分点$s$，使得上式达到最小。

不断地对划分后的子空间按上述方法进行递归划分，直到满足停止条件（例如每个叶子结点仅有一个样本）。

## 3. Boosting提升方法

提升方法是一种非常常见的统计学习方法，通常的思路是**改变训练数据的概率分布（训练数据的权值分布），针对不同训练数据分布调用弱学习方法学习一系列的弱分类器**。

因此针对于Boosting方法，就有两个地方需要注意：

1. 每一轮如何改变训练数据的权值和概率分布
2. 如何将弱分类器组合成一个强分类器

以Boosting算法里常见的AdaBoost算法为例，对于第1个问题，AdaBoost的方法是提升被前一轮弱分类器错误分类的样本的权值，降低被正确分类样本的权值；对于第2个问题，AdaBoost采取加权多数表决的方法，加大分类误差率小的弱分类器的权值，减少分类误差率大的弱分类器的权值。

### 3.1 加法模型与前向分步算法

针对前面两个问题，大部分Boosting算法都是用**加法模型（additive model）**来组合弱分类器，然后用**前向分步算法（forward stagewise algorithm）**来训练改变训练数据的权值和概率分布（包括AdaBoost算法）。

考虑加法模型
$$
f(x)=\sum^M_{m=1}\beta_mb(x;\gamma_m) \label{plus_model}
\\ b(x;\gamma_m)是基函数，\gamma_m基函数的参数，\beta_m是基函数的系数。
$$
在给定训练数据以及损失函数$L(y, f(x))$的条件下，学习加法模型$f(x)$就是一个经验风险极小化问题即损失函数极小化问题
$$
min{\beta_m, \gamma_m}\sum^N_{i=1}L(y_i, \sum^M_{m=1}\beta_m b(x_i;\gamma_m))
\label{loss_func}
$$
直接求解这个的最小值是个很复杂的优化问题，因此我们采用**前向分步算法**来解决这一优化问题。

前向分步算法的思路是：因为学习的是加法模型，若能从前到后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，那么就可以简化复杂度。每步只需要优化如下损失函数
$$
min_{\beta, \gamma}\sum^N_{i=1}L(y_i, \beta b(x_i;\gamma))
$$
这样的话，原本需要同时求解从$m=1$到$M$所有参数$\beta_m,\gamma_m$的优化问题就简化为逐次求解$\beta_m, \gamma_m$的优化问题。

前向分步算法的步骤为
![1](https://raw.githubusercontent.com/Dounm/TheFarmOfDounm/master/resources/images/gdbt/1.png)
（图片来自《统计学习方法》 8.3.1）

### 3.2 Boosting Tree提升树

Boosting方法可以以多种模型作为基本模型，其中以分类决策树或回归决策树为基模型的话被称为**提升树（Boosting Tree）**，是统计学习中性能最好的方法之一[^2]。

按照第1节的说法，提升树实际上采用的是

- 模型：**加法模型**
- 算法：**前向分步算法**
- 策略：可以是平方损失函数，也可以是其他的损失函数。

以平方损失函数和回归决策树为例，推导下提升树的算法。

首先，提升树模型其实就是决策树的加法模型，表现为：
$$
f_M(x)=\sum^M_{m=1}T(x;\Theta_m) \\ 
T(x;\Theta_m)表示决策树；\Theta_m为决策树参数；M为树的个数 \\
$$
注意相比于公式$\ref{plus_model}​$，此处基模型系数默认为1。 [^3]

回归问题提升树的前向分步算法如下：
$$
f_0(x)=0 \\
f_m(x) = f_{m-1}(x) +T(x;\Theta_m), m=1,2,\dots,M \\
f_M(x) = \sum^M_{m=1}T(x;\Theta_m)
$$
在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x)$，需要求解
$$
\hat{\Theta}_m=argmin_{\Theta_m}\sum^N_{i=1}L(y_i, f_{m-1}(x_i)+T(x_i;\Theta_m))
$$
从而得到$\hat{\Theta}_m$，即第$m$棵树的参数。

当采用平方误差损失函数$L(y,f(x))=(y-f(x))^2$时，其损失变为
$$
L(y,f_{m-1}(x)+T(x;\Theta_m)) \\
= [y-f_{m-1}(x)-T(x;\Theta_m)]^2 \\
= [r-T(x;\Theta_m)]^2 \\
其中，r=y-f_{m-1}(x)，即残差（residual）
$$
所以，对于回归问题的Boosting Tree来说，每一步只需要拟合当前模型的残差即可。

### 3.3 Gradient Boosting梯度提升

提升树利用加法模型和前向分步算法实现优化过程时，当损失函数是平方损失函数时，每一步的优化很简单。但对于一般损失函数而言，往往每一步的优化没那么简单，所以引入了**梯度提升（Gradient Boosting）**算法。

梯度提升法利用损失函数的负梯度在当前模型的值$-[\frac{\partial{L(y_i,f(x_i))}}{\partial{f(x_i)}}]_{f(x)=f_{m-1}(x)}$作为之前的残差的近似值，拟合一个回归树。

> 对于任意可导函数$f(x)$来说，$f(x-\epsilon \frac{\partial f(x)}{\partial x}) < f(x)$是必然情况。
针对于$Loss$，它的自变量是$f(x)$，所以沿着负梯度方向更新是合理的。

算法如下：
![2](https://raw.githubusercontent.com/Dounm/TheFarmOfDounm/master/resources/images/gdbt/2.png)
![3](https://raw.githubusercontent.com/Dounm/TheFarmOfDounm/master/resources/images/gdbt/3.png)
（图片来自《统计学习方法》 8.4.3）

[^1]: 详细请参照《统计学习方法》第五章
[^2]: 为什么树模型是最好的
[^3]: 为什么系数默认为1？